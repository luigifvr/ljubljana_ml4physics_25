{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rc_picgAJj5f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data(n_samples):\n",
        "    \"\"\"\n",
        "    Sinusoidal function as defined in D'Angelo et al. 2106.11642\n",
        "    \"\"\"\n",
        "    x = np.concatenate((np.random.uniform(1.5, 2.5, n_samples // 2), np.random.uniform(4.5, 6.0, n_samples - n_samples // 2)))\n",
        "    y = x * np.sin(x) + np.random.normal(0, 0.25, n_samples)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "dOKUKVTOP1tZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = generate_data(128)\n",
        "xx = np.linspace(0, 8)\n",
        "plt.scatter(x, y, marker=\".\", color=\"k\", alpha=0.2)\n",
        "plt.plot(xx, xx*np.sin(xx), color=\"C1\")\n",
        "plt.ylim(-6,4)"
      ],
      "metadata": {
        "id": "xZG-NcJSDSsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Or with more data\n",
        "batch_size = 64\n",
        "n_samples = batch_size*10\n",
        "\n",
        "x, y = generate_data(n_samples)\n",
        "xx = np.linspace(0, 8)\n",
        "plt.scatter(x, y, marker=\".\", color=\"k\", alpha=0.2)\n",
        "plt.plot(xx, xx*np.sin(xx), color=\"C1\")\n",
        "plt.ylim(-6,4)"
      ],
      "metadata": {
        "id": "SpuT9OmyTOuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearBlock(nn.Module):\n",
        "  def __init__(self, in_features, out_features, dropout=0.0):\n",
        "    super().__init__()\n",
        "    #...\n",
        "\n",
        "class GaussianRegressionNet(nn.Module):\n",
        "    \"\"\"\n",
        "    A neural network for regression with a Gaussian likelihood.\n",
        "    Outputs the mean and log variance of the Gaussian distribution.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dims, dropout_rate=0.0):\n",
        "        super().__init__()\n",
        "        #..."
      ],
      "metadata": {
        "id": "mxIHZj0qDbsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset and dataloader\n",
        "dataset = torch.utils.data.TensorDataset(torch.tensor(x[:,None], dtype=torch.float32), torch.tensor(y, dtype=torch.float32))\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "def gaussian_nll(mean, log_var, target):\n",
        "  loss = 0.5 * torch.log(torch.tensor(2 * np.pi)) + log_var + 0.5 * torch.square(mean - target) / torch.exp(log_var)\n",
        "  return loss\n",
        "\n",
        "#..."
      ],
      "metadata": {
        "id": "0Izgb7rkR0k_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After training, you can use the model for prediction\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    xx_tensor = torch.tensor(xx, dtype=torch.float32).unsqueeze(1)\n",
        "    mean_pred, log_var_pred = model(xx_tensor)\n",
        "    mean_pred = mean_pred.squeeze().numpy()\n",
        "    var_pred = torch.exp(log_var_pred).squeeze().numpy()\n",
        "    std_pred = np.sqrt(var_pred)\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(x, y, marker=\".\", color=\"k\", alpha=0.2, label='Data')\n",
        "plt.plot(xx, xx * np.sin(xx), color=\"C1\", label='True function')\n",
        "plt.plot(xx, mean_pred, color=\"C0\", label='Predicted mean')\n",
        "plt.fill_between(xx, mean_pred - std_pred, mean_pred + std_pred, color=\"C0\", alpha=0.2, label='+/- 1 std dev')\n",
        "plt.ylim(-6, 4)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NoQQwhK-SCur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MC Dropout"
      ],
      "metadata": {
        "id": "TYebobvGQ3LL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract predictions using dropout at test time\n",
        "# ...\n",
        "\n",
        "predictions = np.array(predictions)\n",
        "var_pred = torch.exp(log_var_pred_dropout).squeeze().numpy()\n",
        "std_pred = np.sqrt(std_pred)"
      ],
      "metadata": {
        "id": "6vpjdHd1CjcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = np.array(predictions)\n",
        "var_pred = torch.exp(log_var_pred_dropout).squeeze().numpy()\n",
        "std_pred = np.sqrt(std_pred)\n",
        "\n",
        "# Calculate mean and standard deviation across the MC dropout predictions\n",
        "mc_mean = np.mean(predictions, axis=0)\n",
        "mc_std = np.std(predictions, axis=0)\n",
        "\n",
        "# Plotting the results with MC Dropout uncertainty\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(x, y, marker=\".\", color=\"k\", alpha=0.2, label='Data')\n",
        "plt.plot(xx, xx * np.sin(xx), color=\"C1\", label='True function')\n",
        "plt.plot(xx, mc_mean, color=\"C0\", label='MC Dropout Mean')\n",
        "plt.fill_between(xx, mc_mean - mc_std, mc_mean + mc_std, color=\"C0\", alpha=0.2, label='+/- 1 std dev (MC Dropout)')\n",
        "plt.fill_between(xx, mc_mean - std_pred, mc_mean + std_pred, color=\"C1\", alpha=0.2, label='+/- 1 std dev')\n",
        "plt.ylim(-6, 4)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KNRBkj2mQ6Sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variational Inference"
      ],
      "metadata": {
        "id": "I9qBG602ZXaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# A simple VBLinear class  similar to nn.Linear\n",
        "class VBLinear(nn.Module):\n",
        "    # VB -> Variational Bayes\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(VBLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.resample = True\n",
        "        self.bias = Parameter(torch.Tensor(out_features))\n",
        "        self.mu_w = Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.logsig2_w = Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.random = torch.randn_like(self.logsig2_w)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / np.sqrt(self.mu_w.size(1))\n",
        "        self.mu_w.data.normal_(0, stdv)\n",
        "        self.logsig2_w.data.zero_().normal_(-9, 0.001)\n",
        "        self.bias.data.zero_()\n",
        "\n",
        "    # Only missing a forward function and a KL\n",
        "    #..."
      ],
      "metadata": {
        "id": "suD2eFvHZW28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VBLinearBlock(nn.Module):\n",
        "    \"\"\"A simple linear -> relu -> dropout block.\"\"\"\n",
        "    def __init__(self, in_features, out_features, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.linear = VBLinear(in_features, out_features)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.relu(self.linear(x)))\n",
        "\n",
        "class VBGaussianRegressionNet(nn.Module):\n",
        "    \"\"\"\n",
        "    A neural network for regression with a Gaussian likelihood.\n",
        "    Outputs the mean and log variance of the Gaussian distribution.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dims, training_size, dropout_rate=0.0):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.append(VBLinearBlock(prev_dim, hidden_dim, dropout_rate))\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "        # Output layers for mean and log variance\n",
        "        self.mean_layer = VBLinear(prev_dim, 1)\n",
        "        self.log_var_layer = VBLinear(prev_dim, 1)\n",
        "\n",
        "        # also set the training size\n",
        "        # and save the VB layers\n",
        "        # ...\n",
        "\n",
        "\n",
        "    # we need the KL from the bayesian layers to compute the loss function\n",
        "    def KL(self):\n",
        "        kl = 0\n",
        "        for vb_layer in self.vb_layers:\n",
        "            kl += vb_layer.KL()\n",
        "        return kl / self.training_size\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for layer in self.vb_layers:\n",
        "                layer.reset_parameters()\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.net(x)\n",
        "        mean = self.mean_layer(features)\n",
        "        log_var = self.log_var_layer(features)\n",
        "        return mean, log_var"
      ],
      "metadata": {
        "id": "3lWNRLpAZuHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "# Instantiate the model\n",
        "input_dim = 1\n",
        "hidden_dims = [128, 128, 128, 128] # Example hidden dimensions\n",
        "dropout_rate = 0.0 # Example dropout rate\n",
        "\n",
        "dataset = torch.utils.data.TensorDataset(torch.tensor(x[:,None], dtype=torch.float32), torch.tensor(y, dtype=torch.float32))\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "model = VBGaussianRegressionNet(input_dim, hidden_dims, len(dataset), dropout_rate)\n",
        "\n",
        "# Define optimizer\n",
        "num_epochs = 800\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs*len(loader))\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for xb, yb in loader:\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Variational Inference training loop\n",
        "        # ...\n",
        "\n",
        "    if (epoch+1) % 20 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, NLL: {nll.item():.4f}, KL: {kl.item():.4f}')"
      ],
      "metadata": {
        "id": "-1uTEAF6aCPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval() # This time in eval mode\n",
        "\n",
        "n_predictions_vb = 300  # Number of predictions for VB\n",
        "vb_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    xx_tensor = torch.tensor(xx, dtype=torch.float32).unsqueeze(1)\n",
        "    for _ in range(n_predictions_vb):\n",
        "        vb_mean_pred, vb_log_var_pred = model(xx_tensor)\n",
        "        vb_predictions.append(vb_mean_pred.squeeze().numpy())"
      ],
      "metadata": {
        "id": "MSU0v3mLP0TJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vb_predictions = np.array(vb_predictions)\n",
        "vb_variance = np.array(torch.exp(vb_log_var_pred.mean(dim=0)))\n",
        "vb_pred_std = np.sqrt(vb_variance)\n",
        "\n",
        "# Calculate mean and standard deviation across the VB predictions\n",
        "vb_mean = np.mean(vb_predictions, axis=0)\n",
        "vb_std = np.std(vb_predictions, axis=0)\n",
        "\n",
        "# Plotting the results with VB uncertainty\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(x, y, marker=\".\", color=\"k\", alpha=0.2, label='Data')\n",
        "plt.plot(xx, xx * np.sin(xx), color=\"C1\", label='True function')\n",
        "plt.plot(xx, vb_mean, color=\"C0\", label='VB Mean')\n",
        "plt.fill_between(xx, vb_mean - vb_std, vb_mean + vb_std, color=\"C0\", alpha=0.2, label='+/- 1 std dev (VB)')\n",
        "plt.fill_between(xx, vb_mean - vb_pred_std, vb_mean + vb_pred_std, color=\"C1\", alpha=0.2, label='+/- 1 std dev')\n",
        "plt.ylim(-6, 4)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8BvN3qpDcOmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep ensembles"
      ],
      "metadata": {
        "id": "hhqiFNQYDcKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class StackedLinear(nn.Module):\n",
        "    \"\"\"\n",
        "    Efficient implementation of linear layers for ensembles of networks\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features, channels):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.channels = channels\n",
        "        self.weight = nn.Parameter(torch.empty((channels, out_features, in_features)))\n",
        "        self.bias = nn.Parameter(torch.empty((channels, out_features)))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for i in range(self.channels):\n",
        "            torch.nn.init.kaiming_uniform_(self.weight[i], a=math.sqrt(5))\n",
        "            fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(self.weight[i])\n",
        "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
        "            torch.nn.init.uniform_(self.bias[i], -bound, bound)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return torch.baddbmm(self.bias[:,None,:], input, self.weight.transpose(1,2))\n",
        "\n",
        "def kernel(x, y):\n",
        "    \"\"\"\n",
        "    RBF kernel with median estimator\n",
        "    Motivations for the median estimator heuristic\n",
        "    https://arxiv.org/pdf/1707.07269\n",
        "    \"\"\"\n",
        "    channels = len(x)\n",
        "    dnorm2 = (x.reshape(channels,1,-1) - y.reshape(1,channels,-1)).square().sum(dim=2)\n",
        "    sigma = torch.quantile(dnorm2.detach(), 0.5) / (2 * math.log(channels + 1))\n",
        "    return torch.exp(- dnorm2 / (2*sigma))"
      ],
      "metadata": {
        "id": "sGHkjQe7DfdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StackedLinearBlock(nn.Module):\n",
        "    \"\"\"A simple linear -> relu -> dropout block.\"\"\"\n",
        "    def __init__(self, in_features, out_features, channels, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.linear = StackedLinear(in_features, out_features, channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.relu(self.linear(x)))\n",
        "\n",
        "class StackedGaussianRegressionNet(nn.Module):\n",
        "    \"\"\"\n",
        "    A neural network for regression with a Gaussian likelihood.\n",
        "    Outputs the mean and log variance of the Gaussian distribution.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dims, channels, dropout_rate=0.0):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.append(StackedLinearBlock(prev_dim, hidden_dim, channels, dropout_rate))\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "        # Output layers for mean and log variance\n",
        "        self.mean_layer = StackedLinear(prev_dim, 1, channels)\n",
        "        self.log_var_layer = StackedLinear(prev_dim, 1, channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.net(x)\n",
        "        mean = self.mean_layer(features)\n",
        "        log_var = self.log_var_layer(features)\n",
        "        return mean, log_var"
      ],
      "metadata": {
        "id": "pFodWgm7i2xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "# Instantiate the model\n",
        "input_dim = 1\n",
        "hidden_dims = [64, 64, 64, 64] # Example hidden dimensions\n",
        "dropout_rate = 0.0 # Example dropout rate\n",
        "channels = 10\n",
        "\n",
        "dataset = torch.utils.data.TensorDataset(torch.tensor(x[:,None], dtype=torch.float32), torch.tensor(y, dtype=torch.float32))\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "model = StackedGaussianRegressionNet(input_dim, hidden_dims, channels, dropout_rate)\n",
        "\n",
        "# Define optimizer\n",
        "num_epochs = 100\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs*len(loader))\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for xb, yb in loader:\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Repulsive ensemble training loop\n",
        "        # ...\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "id": "RaQkrSvcjGAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  xx_tensor = torch.tensor(xx, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "  # Expand xx_tensor\n",
        "  xx_exp_tensor = xx_tensor.expand(channels, -1, -1)\n",
        "\n",
        "  re_mean_pred, re_log_var_pred = model(xx_exp_tensor)\n",
        "  re_mean_pred = re_mean_pred.squeeze().numpy() # Shape (channels, num_xx)\n",
        "  re_var_pred = torch.exp(re_log_var_pred).squeeze().numpy()\n",
        "\n",
        "  # Calculate the overall mean and variance of the predictions\n",
        "  re_mean = np.mean(re_mean_pred, axis=0)\n",
        "  re_variance_syst = np.mean(re_var_pred, axis=0)\n",
        "  re_variance_stat = np.var(re_mean_pred, axis=0)\n",
        "\n",
        "  re_std_syst = np.sqrt(re_variance_syst)\n",
        "  re_std_stat = np.sqrt(re_variance_stat)"
      ],
      "metadata": {
        "id": "eHNDfl4XQOXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the results for StackedGaussianRegressionNet\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(x, y, marker=\".\", color=\"k\", alpha=0.2, label='Data')\n",
        "plt.plot(xx, xx * np.sin(xx), color=\"C1\", label='True function')\n",
        "plt.plot(xx, re_mean, color=\"C0\", label='Mean')\n",
        "plt.fill_between(xx, re_mean - re_std_stat, re_mean + re_std_stat, color=\"C0\", alpha=0.2, label='+/- 1 std dev (Stat)')\n",
        "plt.fill_between(xx, re_mean - re_std_syst, re_mean + re_std_syst, color=\"C2\", alpha=0.2, label='+/- 1 std dev (Syst)')\n",
        "plt.ylim(-6, 4)\n",
        "plt.legend()\n",
        "plt.title(\"Example 1D regression w/ uncertainties\")\n",
        "plt.savefig(\"regression_net.pdf\", format='pdf')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H4A9K_kJkxcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Food for thought:\n",
        "- What are the trade-offs in terms of computational cost and performance between these methods?\n",
        "- How would the choice of activation function or network architecture impact the uncertainty estimates?\n",
        "- How does the dropout rate affect the uncertainty estimates? How could you systematically explore different dropout rates?\n",
        "- Why are the variational inference uncertainties unreliable? Can you think of possible solutions to improve the predictive uncertaintites?\n",
        "- How does the number of models in the ensemble affect the uncertainty estimates?"
      ],
      "metadata": {
        "id": "obAhYrxytlEW"
      }
    }
  ]
}