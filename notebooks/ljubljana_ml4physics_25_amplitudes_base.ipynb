{"cells":[{"cell_type":"markdown","metadata":{"id":"7S4-X_TYk4g-"},"source":["# Bayesian amplitude regression\n","## Part 1:"]},{"cell_type":"markdown","metadata":{"id":"UBjSaki2k4hG"},"source":["In the lectures we have covered Bayesian networks.\n","Here we'll put this into practice by building a Bayesian neural network in pytorch to perform a regression task on the LHC amplitude for the production of two photons and a gluon in a gluon-gluon collision."]},{"cell_type":"markdown","metadata":{"id":"GV6WJ587rVFn"},"source":["We want to train a network to predict the amplitude for the process $gg\\rightarrow\\gamma\\gamma g$.  So the amplitude depends on the 4-momentum of 5 particles: 2 incoming gluons, 2 outgoing photons, and one outgoing gluon.  \n","\n","The incoming gluons will have no transverse momentum, but their total momentum along the beam pipe is not necessarily zero.\n","\n","The network we will train is a simple fully connected dense network.  This means that the input and output can only be vectors of real numbers.  We want to input the kinematic information on the particles to the network, and train the network to output the corresponding amplitude."]},{"cell_type":"markdown","metadata":{"id":"6Gnn1YJnk4hI"},"source":["We will start by constructing a Bayesian layer in pytorch, and then building the Bayesian loss function.  We will then construct a Bayesian network from these layers, and use it to perform the amplitude regression. We will implement the variational inference and the repulsive ensemble versions of Bayesian neural networks.\n","\n","This notebook is inspired by https://arxiv.org/abs/2412.12069"]},{"cell_type":"markdown","source":["## Motivation\n","\n","Amplitudes are phase-space dependent probabilities whihc are predicted in QFT.\n","Knowing the amplitude is one of the first steps needed to generate collider events in simulators. The first-principled prediction is completely deterministic, therefore there is no systematic noise from measurements in the quantity we are trying to regress.\n","\n","We can exploit this aspect and introduce noise ourselves to test the ability of the neural networks to learn systematic errors, in a similar way to a closure test."],"metadata":{"id":"zJXHRiI-K2a4"}},{"cell_type":"markdown","metadata":{"id":"olyD_hjzk4hJ"},"source":["#### Outline / tasks:\n"," - Imports \\& plotting set-up\n"," - Loading the data\n"," - Visualising the data\n","     - visualise some of the kinematics of the process (transverse momentum of photons/gluons, MET)\n","     - histogram the amplitudes\n"," - Preprocessing the data\n","     - neural networks like $\\mathcal{O}(1)$ numbers\n","     - how should we preprocess the data?\n"," - Datasets and dataloaders\n","     - details are in the pytorch docs\n"," - Building a Bayesian layer\n"," - Constructing the Bayesian loss function\n"," - Building the Bayesian neural network\n"," - Optimising the neural network\n"," - Plot the train and validation losses as a function of the epochs\n"," - Visualize the predictions\n","\n","     \n","Most practical pytorch skills you need for this work is covered in the basics tutorial at https://pytorch.org/tutorials/beginner/basics/intro.html.\n","\n","Please download the training data `tutorial-2-data.zip` and extract it to the folder `data/tutorial-2-data/`."]},{"cell_type":"markdown","metadata":{"id":"1J9uq-pJk4hL"},"source":["If you want to run this tutorial in google colab, you can open a new colab and then upload this file.\n","\n","The data can be downloaded using"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KFrH60g4nYMd"},"outputs":[],"source":["!wget -O tutorial-2-data.zip https://www.dropbox.com/s/n5e66w91rgmbqz2/dlpp-data.zip?dl=0&file_subpath=%2Fdlpp-data%2Ftutorial-2-data\n","!unzip \"tutorial-2-data.zip\"\n","!mkdir tutorial-2-data\n","!mv dlpp-data/tutorial-2-data/* tutorial-2-data/.\n","!rm -r __MACOSX/\n","!rm -r dlpp-data/\n","!ls"]},{"cell_type":"markdown","metadata":{"id":"cBVXl7xOnoHF"},"source":["Make sure you switch to a GPU runtime to fully utilize the colab."]},{"cell_type":"markdown","metadata":{"id":"6uu5VsR3k4hO"},"source":["### Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dWuR0j4xk4hP"},"outputs":[],"source":["import os\n","import sys\n","import random\n","import time\n","import numpy as np\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch import Tensor\n","from torch.nn.parameter import Parameter, UninitializedParameter\n","from torch.nn import functional as F\n","from torch.nn import init\n","from torch.nn import Module"]},{"cell_type":"markdown","metadata":{"id":"15iTnIyQk4hR"},"source":["#### Plotting set-up"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U7X28Kj9k4hT"},"outputs":[],"source":["from matplotlib import pyplot as plt\n","import matplotlib\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","from matplotlib.lines import Line2D\n","from matplotlib.font_manager import FontProperties\n","import matplotlib.colors as mcolors\n","import colorsys\n"]},{"cell_type":"markdown","metadata":{"id":"gdT5pHrHk4hU"},"source":["## Loading the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vEsoBwpfk4hV"},"outputs":[],"source":["trn_dat = np.load(\"tutorial-2-data/trn_dat.npy\")\n","trn_amp = np.load(\"tutorial-2-data/trn_amp.npy\")\n","\n","val_dat = np.load(\"tutorial-2-data/val_dat.npy\")\n","val_amp = np.load(\"tutorial-2-data/val_amp.npy\")\n","\n","tst_dat = np.load(\"tutorial-2-data/tst_dat.npy\")\n","tst_amp = np.load(\"tutorial-2-data/tst_amp.npy\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eO_Mg7Rfk4hW"},"outputs":[],"source":["print(f\"train data shape: {trn_dat.shape}\")\n","print(f\"train amp  shape: {trn_amp.shape}\")\n","print(f\"test  data shape: {tst_dat.shape}\")\n","print(f\"test  amp  shape: {tst_amp.shape}\")\n","print(f\"val   data shape: {val_dat.shape}\")\n","print(f\"val   amp  shape: {val_amp.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"bEBHYJtJTAdA"},"source":["## Visualising the data"]},{"cell_type":"markdown","metadata":{"id":"no0IvRVWTAdB"},"source":["Below we will make some kinematic plots of the events in the training sample.  Note however that these are not the physical distributions we would measure at the LHC!  In our training data each of these events is associated with an amplitude, which tells us the probability that the event will be produced in the gluon-gluon interaction.  So to get the physical distributions these events would need to be 'weighted' by their amplitude.  However, right now we just want to visualise our training dataset to see what preprocessing we should do."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VMRoNCN_TAdB"},"outputs":[],"source":["def get_init_pz( ev ):\n","    return ev[0][3] + ev[1][3]\n","\n","def get_mass( fv ):\n","    msq = np.round( fv[0]**2 - fv[1]**2 - fv[2]**2 - fv[3]**2 , 5 )\n","    if msq>0:\n","        return np.sqrt( msq )\n","    elif msq<0:\n","        raise Exception( \"mass squared is less than zero\" )\n","    else:\n","        return 0\n","\n","def get_pt( fv ):\n","    ptsq = np.round( fv[1]**2 + fv[2]**2 , 5 )\n","    if ptsq>0:\n","        return np.sqrt( ptsq )\n","    elif ptsq<0:\n","        raise Exception( \"$p_T$ squared is less than zero\" )\n","    else:\n","        return 0\n","\n","def get_met( ev ):\n","    return np.abs( np.sum( [ fv[1]+fv[2] for fv in ev ] ) )"]},{"cell_type":"markdown","metadata":{"id":"K99xOoSpTAdB"},"source":["Plotting the initial $p_z$ of the events in the training sample."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pCgcuCqgTAdB"},"outputs":[],"source":["trn_dat_init_pz = []\n","for ev in trn_dat:\n","    trn_dat_init_pz.append( get_init_pz( ev ) )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YzTV3ttPTAdB"},"outputs":[],"source":["fig, axs = plt.subplots( 1, 1, figsize=(7,5) )\n","\n","axs.hist( trn_dat_init_pz, histtype='stepfilled', fill=None )\n","\n","axs.set_xlabel( \"initial $p_z$ (GeV)\")\n","axs.set_ylabel( \"number of events\")\n","\n","fig.tight_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0FGpZgR8TAdB"},"outputs":[],"source":["fig, axs = plt.subplots( 1, 1, figsize=(7,5) )\n","\n","bins=np.logspace(-9, -3, 21)\n","axs.hist( trn_amp, histtype='stepfilled', fill=None, bins=bins )\n","\n","axs.set_yscale( 'log' )\n","axs.set_xscale( 'log' )\n","\n","axs.set_xlabel( \"train amplitudes\" )\n","axs.set_ylabel( \"number of events\" )\n","\n","fig.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"oWPuFFH8TAdC"},"source":["Plotting the leading photon $p_T$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jTVSid-VTAdC"},"outputs":[],"source":["trn_dat_leading_photon_pt = []\n","for ev in trn_dat:\n","    trn_dat_leading_photon_pt.append( get_pt( ev[2] ) )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4eIj3aakTAdC"},"outputs":[],"source":["fig, axs = plt.subplots( 1, 1, figsize=(7,5) )\n","\n","axs.hist( trn_dat_leading_photon_pt, histtype='stepfilled', fill=None )\n","\n","axs.set_yscale( 'log' )\n","\n","axs.set_xlabel( \"leading photon $p_T$ (GeV)\")\n","axs.set_ylabel( \"number of events\" )\n","\n","fig.tight_layout()"]},{"cell_type":"markdown","source":["## Add systematic noise to the data"],"metadata":{"id":"1EJxEEtOMeZq"}},{"cell_type":"markdown","source":["We now add a systematic noise component to the amplitudes. For instance, relative Gaussian noise sampled as $\\epsilon\\sim \\mathcal{N}(0, \\sigma A)$"],"metadata":{"id":"2XrjMiigMoVG"}},{"cell_type":"code","source":["def add_relative_gaussian_noise(amp, relative_std):\n","  \"\"\"\n","  Adds relative Gaussian noise to an array of amplitudes.\n","  \"\"\"\n","  std = amp * relative_std\n","  noise = np.random.normal(loc=0, scale=std, size=amp.shape)\n","  return amp + noise\n","\n","sigma_noise = 0.1\n","trn_amp_amp = add_relative_gaussian_noise(trn_amp, sigma_noise)\n","val_amp_amp = add_relative_gaussian_noise(val_amp, sigma_noise)\n","tst_amp_amp = add_relative_gaussian_noise(tst_amp, sigma_noise)"],"metadata":{"id":"-a6X7tF5Mhmw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sUuKazkXk4hd"},"source":["## Preprocessing the data"]},{"cell_type":"markdown","metadata":{"id":"qTnu7t_gE2B0"},"source":["We will be using a dense network, so the data needs to be in vector format.  We will collapse the data for each event to a single vector of dimension $5\\times4=20$.  The fact that the data is ordered here is important.  To predict the amplitude given the kinematics, the network needs to know which entries correspond to which particles in the process."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1fgebyhOk4he"},"outputs":[],"source":["def get_pt(fv):\n","    \"\"\" returns p_T of given four vector \"\"\"\n","    ptsq = np.round(fv[:, 1]**2 + fv[:, 2]**2, 5)\n","    return np.sqrt(ptsq)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q_xwlmsAk4hf"},"outputs":[],"source":["# index 2 is leading photon, not gluon (which is 4)\n","trn_dat_gluon_pt = get_pt(trn_dat[:, 4])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ou0_Sw_Nk4hg"},"outputs":[],"source":["nev = trn_dat.shape[0]\n","trn_datf = np.reshape(trn_dat, (nev,-1))\n","val_datf = np.reshape(val_dat, (nev,-1))\n","tst_datf = np.reshape(tst_dat, (nev,-1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1BX-eMm9k4hh"},"outputs":[],"source":["trn_datf.shape"]},{"cell_type":"markdown","source":["**Exercise:** After having looked at the data, apply the preprocessing step you consider helpful to improve the training dynamics.\n","Hint: neural networks like numbers of $O(1)$"],"metadata":{"id":"8l8oELCVcwki"}},{"cell_type":"code","source":["# Your code here..."],"metadata":{"id":"hnZPRVyedLx-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fsEs6lgFk4hq"},"source":["## Datasets and dataloaders"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eoCKI0aDk4hr"},"outputs":[],"source":["class amp_dataset(Dataset):\n","\n","    def __init__(self, data, amp):\n","        self.data = data\n","        self.amp = amp\n","\n","    def __len__(self):\n","        return len(self.amp)\n","\n","    def __getitem__(self, idx):\n","        return self.data[idx], self.amp[idx]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wKLvLls9k4hr"},"outputs":[],"source":["trn_dataset = amp_dataset(trn_datfp, trn_amplp.unsqueeze(-1))\n","val_dataset = amp_dataset(val_datfp, val_amplp.unsqueeze(-1))\n","tst_dataset = amp_dataset(tst_datfp, tst_amplp.unsqueeze(-1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fPwl52C1k4hs"},"outputs":[],"source":["trn_dataloader = DataLoader(trn_dataset, batch_size=64, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n","tst_dataloader = DataLoader(tst_dataset, batch_size=64, shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"RZd3Bu67k4ht"},"source":["## Building a Bayesian layer"]},{"cell_type":"markdown","metadata":{"id":"_gvq4bjXk4hu"},"source":["First let's look at the source code for a **basic linear layer** in pytorch:\n","\n","(https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear)"]},{"cell_type":"markdown","metadata":{"id":"aDJSzzkqk4hu"},"source":["```\n","class Linear(Module):\n","    \n","    __constants__ = ['in_features', 'out_features']\n","    in_features: int\n","    out_features: int\n","    weight: Tensor\n","\n","    def __init__(self, in_features: int, out_features: int, bias: bool = True,\n","                 device=None, dtype=None) -> None:\n","        factory_kwargs = {'device': device, 'dtype': dtype}\n","        super(Linear, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n","        if bias:\n","            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n","        else:\n","            self.register_parameter('bias', None)\n","        self.reset_parameters()\n","\n","    def reset_parameters(self) -> None:\n","        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n","        if self.bias is not None:\n","            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n","            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n","            init.uniform_(self.bias, -bound, bound)\n","\n","    def forward(self, input: Tensor) -> Tensor:\n","        return F.linear(input, self.weight, self.bias)\n","\n","    def extra_repr(self) -> str:\n","        return 'in_features={}, out_features={}, bias={}'.format(\n","            self.in_features, self.out_features, self.bias is not None\n","        )\n","```"]},{"cell_type":"markdown","metadata":{"id":"jlKfJsj1k4hv"},"source":["Objects of this class apply a linear transformation to the incoming data: $y = xA^T + b$.\n","\n","The input arguments are required to initialise the layer, so, in the \\_\\_init()\\_\\_ function.  We have:\n","- in_features: size of each input sample\n","- out_features: size of each output sample\n","- bias: If set to ``False``, the layer will not learn an additive bias.  Default: ``True``\n","\n","The shapes are:\n","- Input: $(*, H_{in})$ where $*$ means any number of dimensions including none and $H_{in} = \\text{in_features}$.\n","- Output: $(*, H_{out})$ where all but the last dimension are the same shape as the input and $H_{out} = \\text{out_features}$.\n","\n","The layer has attributes:\n","- weight: the learnable weights of the module of shape $(\\text{out_features}, \\text{in_features})$. The values are initialized from $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$, where $k = \\frac{1}{\\text{in_features}}$\n","- bias:   the learnable bias of the module of shape $(\\text{out_features})$.  If `bias` is ``True``, the values are initialized from $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$ where $k = \\frac{1}{\\text{in_features}}$.\n","\n","Examples::\n","\n","    >>> m = nn.Linear(20, 30)\n","    \n","    >>> input = torch.randn(128, 20)\n","    \n","    >>> output = m(input)\n","    \n","    >>> print(output.size())\n","    \n","    torch.Size([128, 30])"]},{"cell_type":"markdown","metadata":{"id":"ogno-VAcBRKy"},"source":["From the lecture, we know that in a Bayesian network the weights are replaced by Gaussian distributions, and on a forward pass we get the output by sampling from that distribution.\n","\n","So the biases are the same as in the linear layer.  But not each weight is a Gaussian distibution and so needs a mean and a variance.  The bias and the mean and variance of the weight distribution will be learnable.  In practice it's easier to work with the log of the variance as it is more stable when optimising the network.\n","\n","We need to be able to sample from the Gaussian weight distributions, and compute derivatives of the output in order to update the network parameters. To do this we use something called the 're-parameterisation trick'. It involves sampling random noise from a unit normal distribution, and then transforming that number using the mean and variance of the weight distribution in this way:\n","\\begin{equation}\n","w = \\mu + \\sigma\\times r\n","\\end{equation}\n","\n","where $r$ is a random number sampled from a unit normal distribution (Gaussian with mean and variance equal to one), $\\mu$ is the mean of the weight distribution, and $\\sigma$ is the standard deviation. In this way we separate the stochastic part of the function from the parameters defining the distribution. And so if we take any differentiable function of $x$ (e.g. an activation function), we can compute derivatives of that function with respect to the mean and variance of the weight distribution.\n","\n","In the `forward` method we then need to implement this reparameterisation trick for the weights, and then apply the same linear transformation as in the standard linear layer.\n","\n","On each forward pass we need to generate a set of random numbers with the same shape as our means and variances.  Choosing a set of random numbers for the sampling is equivalent to 'sampling' a new neural network from the Bayesian neural network.  And sometimes at the end of the analysis, we will want to keep the same network for testing.  So we don't always want to re-sample the random numbers on each forward pass.  To control this we define a function called `reset_random` which allows to easily sample a new state.\n","\n","We also need a `reset_parameters` function to reset the parameters in the network.  This is standard for layers in pytorch.  We use a slightly different function to do this than is used in the pytorch linear layer, as can be seen below.\n","\n","From the lecture, you know that the weight distributions require a prior.  The simplest choice for this prior is just a Gaussian distribution with mean zero and variance of one.  Results are typically not too sensitive to this prior, as long as the values are within reasonable limits.  For example, $\\mathcal{O}(1)$ means and standard deviations.  Going beyond $\\mathcal{O}(1)$ numbers just leads to numerical instabilities in the training.  The Bayesian loss function contains a term coming from the KL divergence between the weight distributions in the network and their priors.  So the layers should have some funcitonality to return these values.\n","\n","We also introduce the `map` option, which sets the weights to the learned mean without any sampling."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KaPF-EDnk4hx"},"outputs":[],"source":["import math\n","\n","class VBLinear(nn.Module):\n","    def __init__(self, in_features, out_features, prior_prec=1.0, _map=False, std_init=-9):\n","        super(VBLinear, self).__init__()\n","        self.n_in = in_features\n","        self.n_out = out_features\n","        self.map = _map\n","        self.prior_prec = prior_prec\n","        self.random = None\n","        self.bias = nn.Parameter(torch.Tensor(out_features))\n","        self.mu_w = nn.Parameter(torch.Tensor(out_features, in_features))\n","        self.logsig2_w = nn.Parameter(torch.Tensor(out_features, in_features))\n","        self.std_init = std_init\n","        self.reset_parameters()\n","\n","    def enable_map(self):\n","        self.map = True\n","\n","    def disenable_map(self):\n","        self.map = False\n","\n","    def reset_parameters(self):\n","        stdv = 1. / math.sqrt(self.mu_w.size(1))\n","        self.mu_w.data.normal_(0, stdv)\n","        self.logsig2_w.data.zero_().normal_(self.std_init, 0.001)\n","        self.bias.data.zero_()\n","\n","    def reset_random(self):\n","        self.random = None\n","        self.map = False\n","\n","    def sample_random_state(self):\n","        return torch.randn_like(self.logsig2_w).detach().cpu().numpy()\n","\n","    def import_random_state(self, state):\n","        self.random = torch.tensor(state, device=self.logsig2_w.device,\n","                                   dtype=self.logsig2_w.dtype)\n","\n","    def KL(self):\n","        logsig2_w = self.logsig2_w.clamp(-30, 11)\n","        kl = 0.5 * (self.prior_prec * (self.mu_w.pow(2) + logsig2_w.exp())\n","                    - logsig2_w - 1 - math.log(self.prior_prec)).sum()\n","        return kl\n","\n","    def forward(self, input):\n","        if self.training:\n","            # local reparameterization trick is more efficient and leads to\n","            # an estimate of the gradient with smaller variance.\n","            # https://arxiv.org/pdf/1506.02557.pdf\n","            mu_out = F.linear(input, self.mu_w, self.bias)\n","            logsig2_w = self.logsig2_w.clamp(-30, 11)\n","            s2_w = logsig2_w.exp()\n","            var_out = F.linear(input.pow(2), s2_w) + 1e-12 # Needed to avoid NaNs from gradient of sqrt in next line!\n","            return mu_out + var_out.sqrt() * torch.randn_like(mu_out)\n","\n","        else:\n","            if self.map:\n","                return F.linear(input, self.mu_w, self.bias)\n","\n","            logsig2_w = self.logsig2_w.clamp(-30, 11)\n","            if self.random is None:\n","                self.random = torch.randn_like(self.mu_w)\n","            s2_w = logsig2_w.exp()\n","            weight = self.mu_w + s2_w.sqrt() * self.random\n","            return F.linear(input, weight, self.bias) #+ 1e-8\n","\n","    def __repr__(self):\n","        return f\"{self.__class__.__name__} ({self.n_in}) -> ({self.n_out})\""]},{"cell_type":"markdown","source":["You can also find this class in the GitHub repository and simply import it in your code."],"metadata":{"id":"VBHvQ6NbkR_j"}},{"cell_type":"markdown","metadata":{"id":"O2611iYyk4hy"},"source":["Let's test the layers, define:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OC-6EqnNk4hy"},"outputs":[],"source":["tlr0 = VBLinear(10, 5)\n","tlr1 = VBLinear(5, 2)"]},{"cell_type":"markdown","metadata":{"id":"TEwfrf0Qk4hz"},"source":["Now some test data, a batch of 20 vectors with 10 elements each:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RVk-oE-Mk4h0"},"outputs":[],"source":["x = torch.rand(20, 10)"]},{"cell_type":"markdown","metadata":{"id":"tdkwCLnPk4h0"},"source":["Now pass the data first through layer0 then through layer1:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UF38n5ZXk4h1"},"outputs":[],"source":["tlr1(tlr0(x))"]},{"cell_type":"markdown","metadata":{"id":"x6ZuuyG5k4h2"},"source":["Note this has the correct shape:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eQQNBPJ_k4h2"},"outputs":[],"source":["tlr1(tlr0(x)).shape"]},{"cell_type":"markdown","metadata":{"id":"vXcyfd3Uk4h3"},"source":["Also try running the same data through the layer multiple times, you get different results.  This is because of the sampling in the Bayesian layer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kIy9SDl9k4h3"},"outputs":[],"source":["for i in range(5):\n","    print(tlr0(x)[0][0])"]},{"cell_type":"markdown","metadata":{"id":"URkyV79Pk4h5"},"source":["## The Bayesian loss function"]},{"cell_type":"markdown","metadata":{"id":"JexlT6qtk4h6"},"source":["From the lectures we know that there are two parts to the Bayesian loss function:\n","- The negative log Gaussian\n","- the KL from the network weights\n","\n","The second comes from the layers, and the first is defined below.  This negative log Gaussian term acts on the two outputs from the Bayesian neural network:\n","- the mean\n","- the variance (which we parameterise as the log variance)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aSuUV6i1k4h7"},"outputs":[],"source":["def neg_log_gauss(outputs, targets):\n","\n","    mu = outputs[:, 0]\n","    logsigma2 = outputs[:, 1]\n","    out = torch.pow(mu - targets, 2) / (2 * logsigma2.exp()) + 1./2. * logsigma2\n","\n","    return torch.mean(out)"]},{"cell_type":"markdown","metadata":{"id":"pIxAJ2W9k4iB"},"source":["## Building the Bayesian neural network"]},{"cell_type":"markdown","metadata":{"id":"8Fa1G-mXk4iC"},"source":["We'll build a simple network with one input and one output layer, and two hidden layers.  We define the dimensions of these layers below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1r_2vxRGk4iD"},"outputs":[],"source":["ipt_dim = 20\n","opt_dim = 1\n","hdn_dim = 50"]},{"cell_type":"markdown","metadata":{"id":"TnOcHgvEk4iF"},"source":["We can now use the same building blocks we already implemented in the first session."]},{"cell_type":"code","source":["class VBLinearBlock(nn.Module):\n","    \"\"\"A simple linear -> relu -> dropout block.\"\"\"\n","    def __init__(self, in_features, out_features, dropout_rate=0.1):\n","        super().__init__()\n","        self.linear = VBLinear(in_features, out_features)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","    def forward(self, x):\n","        return self.dropout(self.relu(self.linear(x)))\n","\n","class BayesianAmpNet(nn.Module):\n","    \"\"\"\n","    A neural network for regression with a Gaussian likelihood.\n","    Outputs the mean and log variance of the Gaussian distribution.\n","    \"\"\"\n","    def __init__(self, input_dim, hidden_dims, training_size, dropout_rate=0.0):\n","        super().__init__()\n","        layers = []\n","        prev_dim = input_dim\n","        for hidden_dim in hidden_dims:\n","            layers.append(VBLinearBlock(prev_dim, hidden_dim, dropout_rate))\n","            prev_dim = hidden_dim\n","\n","        self.net = nn.Sequential(*layers)\n","\n","        # Output layers for mean and log variance\n","        self.mean_layer = VBLinear(prev_dim, 1)\n","        self.log_var_layer = VBLinear(prev_dim, 1)\n","\n","        self.training_size = training_size\n","        self.vb_layers = []\n","        self.vb_layers.append(self.mean_layer)\n","        self.vb_layers.append(self.log_var_layer)\n","        for layer in self.net:\n","            if isinstance(layer, VBLinear):\n","                self.vb_layers.append(layer)\n","\n","    # we need the KL from the bayesian layers to compute the loss function\n","    def KL(self):\n","        kl = 0\n","        for vb_layer in self.vb_layers:\n","            kl += vb_layer.KL()\n","        return kl / self.training_size\n","\n","    # let's put the neg_log_gauss in the network class aswell since it is key to bayesian networks\n","    def neg_log_gauss(self, outputs, targets):\n","        mu = outputs[0].squeeze(-1)\n","        logsigma2 = outputs[1].squeeze(-1)\n","        out = torch.pow(mu - targets, 2) / (2 * logsigma2.exp()) + 1./2. * logsigma2\n","        return torch.mean(out)\n","\n","    def reset_random(self):\n","        for layer in self.vb_layers:\n","                layer.reset_random()\n","\n","    def forward(self, x):\n","        features = self.net(x)\n","        mean = self.mean_layer(features)\n","        log_var = self.log_var_layer(features)\n","        return mean, log_var"],"metadata":{"id":"GR6ZxN5FJZWh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dn7B1pfQk4iH"},"source":["Check if we have a GPU."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oq_Vkanyk4iJ"},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using {device} device\")"]},{"cell_type":"markdown","metadata":{"id":"_xonu5S2k4iK"},"source":["Initialise the neural network and send to the GPU if we have one.  We can also print the model to see what layers it has."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZYWui0urk4iL"},"outputs":[],"source":["# Define your favorite neural network here ...\n","# then test it. In particular verify that each call\n","# gives different outputs, also with the same input."]},{"cell_type":"markdown","metadata":{"id":"yDaOlWYvk4iY"},"source":["## Optimising (training) the neural network"]},{"cell_type":"markdown","metadata":{"id":"3HGQ4uRZk4iZ"},"source":["The Bayesian loss function has two terms which we have already definedl; the negative los Gaussian, and the KL divergence between the network and the network prior.  The latter comes from the KL divergence over the weights in the network."]},{"cell_type":"markdown","metadata":{"id":"Tztwep9qk4ia"},"source":["Now we can write a training loop for a single epoch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mauiw9TQk4ia"},"outputs":[],"source":["def train_epoch(dataloader, model, optimizer, scheduler):\n","\n","    size = len(dataloader.dataset)\n","    model.train()\n","    loss_tot, kl_tot, neg_log_tot = 0.0, 0.0, 0.0\n","    loss_during_opt, kl_during_opt, neg_log_during_opt = 0., 0., 0.\n","\n","    for batch, (X, y) in enumerate(dataloader):\n","\n","        # Define a training loop which gives predictions\n","        # calculates the nll, the kl, and updates the network\n","\n","        # print the training loss every 100 updates\n","        if batch % 100 == 0:\n","            current = batch * len( X )\n","            print(f\"current batch loss: {loss:>8f} KL: {kl:>8f} Neg-log {nl:>8f}  [{current:>5d}/{size:>5d}]\")\n","    loss_live = loss_during_opt/len(dataloader)\n","    kl_live = kl_during_opt / len(dataloader)\n","    nl_live = neg_log_during_opt / len(dataloader)\n","\n","    return loss_live, kl_live, nl_live"]},{"cell_type":"markdown","metadata":{"id":"jedz0a--k4ib"},"source":["To monitor the performance of the network on the regression task we want to calculate the loss of both the training data and the validation data on the same network, so we have the following functions:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"psVdK6Dek4ic"},"outputs":[],"source":["def val_pass(dataloader, model):\n","\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    nls = 0.0\n","    kls = 0.0\n","    vls = 0.0\n","    mse = 0.0\n","\n","    # we don't need gradients here since we only use the forward pass\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            pred = model(X)\n","            nl = model.neg_log_gauss(pred, y.reshape(-1))\n","            kl = model.KL()\n","            vl = nl.item() + kl.item()\n","            mse += torch.mean((pred[0].squeeze(-1) - y.reshape(-1))**2)\n","            nls += nl\n","            kls += kl\n","            vls += vl\n","\n","    nls /= num_batches\n","    kls /= num_batches\n","    vls /= num_batches\n","    mse /= num_batches\n","    print(f'Val loss: {vls:.4f}, NLL: {nls:.4f}, KL: {kls:.4f}, MSE {mse:>8}')\n","\n","    return nls.cpu().numpy(), kls.cpu().numpy(), vls, mse.cpu().numpy()\n","\n","def trn_pass(dataloader, model):\n","\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    nls = 0.0\n","    kls = 0.0\n","    tls = 0.0\n","    mse = 0.0\n","\n","    # we don't need gradients here since we only use the forward pass\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            pred = model(X)\n","            nl = model.neg_log_gauss(pred, y.reshape(-1))\n","            kl = model.KL()\n","            tl = nl.item() + kl.item()\n","            mse += torch.mean((pred[0].squeeze(-1) - y.reshape(-1))**2)\n","            nls += nl\n","            kls += kl\n","            tls += tl\n","\n","    nls /= num_batches\n","    kls /= num_batches\n","    tls /= num_batches\n","    mse /= num_batches\n","    print( f\"avg trn loss per batch: {tls:>8f} KL: {kls:>8f} Neg-log {nls:>8f} MSE {mse:>8}\" )\n","\n","    return nls.cpu().numpy(), kls.cpu().numpy(), tls, mse.cpu().numpy()"]},{"cell_type":"markdown","metadata":{"id":"ZF-N4BL3k4id"},"source":["Now we can train the model!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rM4uEcWfk4ie","scrolled":true},"outputs":[],"source":["# a useful function to present things clearer\n","def seperator():\n","    print( \"-----------------------------------------------\" )\n","\n","# Set a batch size\n","# define train, test, and validation dataset and dataloader\n","# you code here...\n","\n","\n","# re-initialise the model, the optimizer, and th scheduler\n","# finally train the network"]},{"cell_type":"code","source":["# track train and val losses\n","trn_nl_losses = []\n","trn_kl_losses = []\n","trn_losses = []\n","trn_mse_losses = []\n","val_nl_losses = []\n","val_kl_losses = []\n","val_losses = []\n","val_mse_losses = []\n","trn_nl_losses_live = []\n","trn_kl_losses_live = []\n","trn_losses_live = []\n","\n","for t in range(epochs):\n","    seperator()\n","    print(f\"Epoch {t+1}\")\n","    seperator()\n","\n","    loss_live, kl_live, nl_live = train_epoch(trn_dataloader, model_bnn, optimizer, scheduler)\n","    trn_nl_losses_live.append(nl_live)\n","    trn_kl_losses_live.append(kl_live)\n","    trn_losses_live.append(loss_live)\n","    seperator()\n","\n","    trn_nl_loss, trn_kl_loss, trn_loss, trn_mse_loss = trn_pass(trn_dataloader, model_bnn)\n","    trn_nl_losses.append(trn_nl_loss)\n","    trn_kl_losses.append(trn_kl_loss)\n","    trn_losses.append(trn_loss)\n","    trn_mse_losses.append(trn_mse_loss)\n","    seperator()\n","\n","    val_nl_loss, val_kl_loss, val_loss, val_mse_loss = val_pass(val_dataloader, model_bnn)\n","    val_nl_losses.append(val_nl_loss)\n","    val_kl_losses.append(val_kl_loss)\n","    val_losses.append(val_loss)\n","    val_mse_losses.append(val_mse_loss)\n","    seperator()\n","    print( \"|\" )\n","\n","print(\"Done!\")"],"metadata":{"id":"KU0Fxbi_u-tf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aX_EYEu2k4if"},"source":["## Plot the train and validation losses as a function of the epochs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FyJfasl8k4if"},"outputs":[],"source":["fig, axs = plt.subplots(1, 4, figsize=(18,5))\n","\n","c1 = 'tab:red'\n","c2 = 'tab:green'\n","\n","axs[0].plot(trn_losses, label=\"train\", color=c1)\n","axs[0].plot(val_losses, label=\"val\", color=c2)\n","axs[0].plot(trn_losses_live, label=\"train live\", color=c1, ls='dashed')\n","axs[0].set_xlabel(\"epoch\")\n","axs[0].set_ylabel(\"Total loss\")\n","axs[0].set_ylim(-3, 1)\n","axs[0].legend()\n","\n","axs[1].plot(trn_nl_losses, label=\"train\", color=c1)\n","axs[1].plot(val_nl_losses, label=\"val\", color=c2)\n","axs[1].plot(trn_nl_losses_live, label=\"train live\", color=c1, ls='dashed')\n","axs[1].set_xlabel(\"epoch\")\n","axs[1].set_ylabel(\"Neg-log-gauss loss\")\n","axs[1].set_ylim(-3, 1)\n","axs[1].legend()\n","\n","axs[2].plot(trn_kl_losses, label=\"train\", color=c1)\n","axs[2].plot(val_kl_losses, label=\"val\", color=c2)\n","axs[2].plot(trn_kl_losses_live, label=\"train live\", color=c1, ls='dashed')\n","axs[2].set_yscale('log')\n","axs[2].set_xlabel(\"epoch\")\n","axs[2].set_ylabel(\"KL loss\")\n","axs[2].legend()\n","\n","\n","axs[3].plot(trn_mse_losses, label=\"train\", color=c1)\n","axs[3].plot(val_mse_losses, label=\"val\", color=c2)\n","axs[3].set_yscale('log')\n","axs[3].set_xlabel(\"epoch\")\n","axs[3].set_ylabel(\"MSE Loss\")\n","axs[3].legend()\n","\n","fig.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"vFV-TKWqk4ii"},"source":["We can see that both the train and validation losses are being reduced during training, the model is fitting well!"]},{"cell_type":"markdown","source":["#Part 2:\n","\n","#### Outline / tasks:\n","- Repulsive Ensembles\n","- Study the results\n","  - Evaluate accuracy of the model\n","  - Calculate uncertainties\n","  - Visualize predicted amplitudes w/ uncertainties\n","  - Plot pull distributions\n","  - Discuss calibration of predicted uncertainties\n","- Questions/Exercises"],"metadata":{"id":"5El3RFK9flNC"}},{"cell_type":"code","source":["# Using the code from the first session and the previous part\n","# now train a respulsive ensemble using a similar network\n","\n","# Hint: you can reuse the StackedLinear layers\n","# and the StackedLinearBlock from the previous session"],"metadata":{"id":"PYaW-yGvvC-z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TBU_mmBatloS"},"source":["## Plot the train and validation losses as a function of the epochs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bAwZWzPltloW"},"outputs":[],"source":["fig, axs = plt.subplots(1, 3, figsize=(13,5))\n","\n","c1 = 'tab:red'\n","c2 = 'tab:green'\n","\n","axs[0].plot(trn_losses, label=\"train\", color=c1)\n","axs[0].plot(val_losses, label=\"val\", color=c2)\n","axs[0].set_xlabel(\"epoch\")\n","axs[0].set_ylabel(\"Total loss\")\n","axs[0].set_ylim(-3.5, 1)\n","axs[0].legend()\n","\n","axs[1].plot(trn_nl_losses, label=\"train\", color=c1)\n","axs[1].plot(val_nl_losses, label=\"val\", color=c2)\n","axs[1].set_xlabel(\"epoch\")\n","axs[1].set_ylabel(\"Neg-log-gauss loss\")\n","axs[1].set_ylim(-3.5, 1)\n","axs[1].legend()\n","\n","axs[2].plot(trn_mse_losses, label=\"train\", color=c1)\n","axs[2].plot(val_mse_losses, label=\"val\", color=c2)\n","axs[2].set_yscale('log')\n","axs[2].set_xlabel(\"epoch\")\n","axs[2].set_ylabel(\"MSE Loss\")\n","axs[2].legend()\n","\n","fig.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"yUDOhSlUmis6"},"source":["## Study the results\n","\n","We will now discuss how to analyse the outputs of the Bayesian network, and how this gives us the ability to estimate the error on our analysis. This last step is crucial to the application of any numerical technique in physics."]},{"cell_type":"markdown","metadata":{"id":"atjIHqPTmis8"},"source":["Now we want to get some visualisation of how well our amplitude regression has worked.\n","\n","The simplest thing we can do is to pass our data through the neural network to get a predicted amplitude for each event, then histogram this and compare it to the histogram of the true amplitudes."]},{"cell_type":"code","source":["# Define two fuctions for the extraction of the predictions\n","# and the learned uncertainties from the VI and RE\n","def get_bnn_predictions(model, dataloader, n_monte=30):\n","    \"\"\"\n","    Gets predictions from BayesianAmpNet for a given number of Monte Carlo samples.\n","    \"\"\"\n","    with torch.no_grad():\n","        amps_samples = []\n","        sigma2_samples = []\n","\n","        # Your code here...\n","\n","    return amps_samples, sigma2_samples\n","\n","def get_repulsive_predictions(model, dataloader):\n","    \"\"\"\n","    Gets predictions from the repulsive ensemble\n","    For repulsive ensembles, the predictions are the outputs of each ensemble member.\n","    \"\"\"\n","    with torch.no_grad():\n","        amps_samples = []\n","        sigma2_samples = []\n","\n","        # Your code here...\n","\n","    return amps_samples, sigma2_samples\n","\n","def get_predictions(model, dataloader, n_monte=30):\n","    \"\"\"\n","    Gets predictions for a given number of Monte Carlo times for\n","    both variational inference and repulsive ensemble.\n","\n","    Args:\n","        model: A torch nn.Module\n","        dataloader: DataLoader for the data.\n","        n_monte: Number of Monte Carlo samples (only used for var. inference).\n","\n","    Returns:\n","        A tuple containing:\n","            - amps_samples: numpy array of predicted means.\n","            - sigma2_samples: numpy array of predicted variances.\n","    \"\"\"\n","\n","    # Your code here..."],"metadata":{"id":"hgruPWx468uC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tst_dataloader = DataLoader(tst_dataset, batch_size=64, shuffle=False)\n","\n","# Get predictions from the VI BNN\n","bnn_amps, bnn_sigma2 = get_predictions(model_bnn, tst_dataloader, n_monte=10)\n","print(f\"BayesianAmpNet predictions shape: mean={bnn_amps.shape}, variance={bnn_sigma2.shape}\")\n","\n","# Get predictions from the repulsive ensemble\n","repulsive_amps, repulsive_sigma2 = get_predictions(model_re, tst_dataloader) # n_monte is ignored\n","print(f\"BayesianAmpNetV2 predictions shape: mean={repulsive_amps.shape}, variance={repulsive_sigma2.shape}\")\n","\n","print(\"Mean bnn sigma2: \", np.mean(bnn_sigma2))\n","print(\"Mean re sigma2: \", np.mean(repulsive_sigma2))"],"metadata":{"id":"Ur_cQyrp6GjZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# First evaluate the accuracy of our model,\n","# calculate negative log likelihoods\n","\n","bnn_nll =\n","\n","repulsive_nll =\n","\n","print(\"Negative Log Likelihood (VI):\", bnn_nll)\n","print(\"Negative Log Likelihood (Repulsive Ensemble):\", repulsive_nll)\n","\n","# Then, look at the predicted uncertainties,\n","# calculate total, systematic, and statistical variance\n","bnn_sigma2_syst =\n","bnn_sigma2_stat =\n","bnn_sigma2_tot =\n","\n","bnn_sigma_syst =\n","bnn_sigma_stat =\n","bnn_sigma_tot =\n","\n","re_sigma2_syst =\n","re_sigma2_stat =\n","re_sigma2_tot =\n","\n","re_sigma_syst =\n","re_sigma_stat =\n","re_sigma_tot =\n","\n","bnn_delta =\n","re_delta ="],"metadata":{"id":"5z35mRXPgVUK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Histograms of uncertainties (Syst, Stat, Total)\n","\n","fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n","bins_unc = np.logspace(-3, 1, 50)\n","ax[0].hist(bnn_sigma_syst, bins=bins_unc, histtype='step', label='Systematic')\n","ax[0].hist(bnn_sigma_stat, bins=bins_unc, histtype='step', label='Statistical')\n","ax[0].hist(bnn_sigma_tot, bins=bins_unc, histtype='stepfilled', fill=True, alpha=0.3, label='Total')\n","ax[0].set_xscale('log')\n","ax[0].set_yscale('log')\n","ax[0].set_xlabel(\"$\\sigma$\")\n","ax[0].set_ylabel(\"Number of Events\")\n","ax[0].set_title(\"BNN\")\n","ax[0].legend()\n","\n","ax[1].hist(re_sigma_syst, bins=bins_unc, histtype='step', label='Systematic')\n","ax[1].hist(re_sigma_stat, bins=bins_unc, histtype='step', label='Statistical')\n","ax[1].hist(re_sigma_tot, bins=bins_unc, histtype='stepfilled', fill=True, alpha=0.3, label='Total')\n","ax[1].set_xscale('log')\n","ax[1].set_yscale('log')\n","ax[1].set_xlabel(\"$\\sigma$\")\n","ax[1].set_ylabel(\"Number of Events\")\n","ax[1].set_title(\"Repulsive Ensemble\")\n","ax[1].legend()"],"metadata":{"id":"XwdbbsyeQSAL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Histograms of accuracies (Syst, Stat, Total)\n","\n","fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n","bins_delta = np.linspace(-3, 3, 50)\n","ax[0].hist(bnn_delta, bins=bins_delta, histtype='step', label='Rel. accuracy')\n","ax[0].set_yscale('log')\n","ax[0].set_xlabel(\"$\\Delta$\")\n","ax[0].set_ylabel(\"Number of Events\")\n","ax[0].set_title(\"BNN\")\n","ax[0].legend()\n","\n","ax[1].hist(re_delta, bins=bins_delta, histtype='step', label='Rel. accuracy')\n","ax[1].set_yscale('log')\n","ax[1].set_xlabel(\"$\\Delta$\")\n","ax[1].set_ylabel(\"Number of Events\")\n","ax[1].set_title(\"Repulsive Ensemble\")\n","ax[1].legend()"],"metadata":{"id":"hYVpA19BS49M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Here we show an histogram of the predicted amplitudes with uncertainties\n","\n","bins=np.linspace(-3, 3, 21)\n","values, true_test_bin = [], []\n","pred_reps =[]\n","n_rep = 100\n","for _ in range(n_rep):\n","    error = np.random.normal(loc=0.0, scale=bnn_sigma_tot, size=bnn_amps.shape[-1])\n","    data_rep = error + np.mean(bnn_amps, axis=0)\n","    pred_reps.append(data_rep)\n","pred_reps = np.stack(pred_reps, 0)\n","print(\"Shape of data w/ replicas: \", pred_reps.shape)\n","\n","# Get histogram values\n","for i in range(len(pred_reps)):\n","    hist, bin_edges = np.histogram(pred_reps[i], bins=bins)\n","    # Normalize by the number of amplitudes\n","    hist = hist.astype('float')/hist.sum()\n","    values.append(hist)\n","bin_center = (bin_edges[:-1] + bin_edges[1:]) / 2\n","values = np.stack(values, axis=1)\n","mean_hist = np.mean(values, axis=1)\n","std_hist = np.std(values, axis=1)\n","true_hist, bin_edges = np.histogram(tst_ampl, bins=bins)\n","true_hist = true_hist.astype('float')/true_hist.sum()\n","pred_ens_hist, bin_edges = np.histogram(tst_ampl, bins=bins)\n","pred_ens_hist = pred_ens_hist.astype('float')/pred_ens_hist.sum()"],"metadata":{"id":"EzI8Nm-SWzhM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, ax = plt.subplots(3,1,figsize=[5,5], sharex=True, gridspec_kw={'height_ratios':(4,1,1), \"hspace\": 0.0})\n","\n","# add ratio plot for test/true\n","ax[0].step(bin_edges[:-1], true_hist, where='post', label='True', color='C3')\n","ax[0].step(bin_edges[:-1], mean_hist, where='post', label=\"Mean\", color='C1')\n","ax[0].fill_between(bin_edges[:-1], mean_hist+std_hist, mean_hist-std_hist, step=\"post\", alpha=.3, color='C1')\n","ax[0].set_yscale( 'log' )\n","ax[0].legend()\n","\n","ax[1].set_ylabel(r'$\\frac{\\text{Model}}{\\text{Truth}}$')\n","ax[1].set_ylim(.7,1.3)\n","ax[1].get_xticklabels()\n","\n","ratio = mean_hist/true_hist\n","ratio_isnan = np.isnan(ratio)\n","ratio[ratio_isnan] = 1.\n","delta = np.fabs(ratio - 1)*100\n","\n","ax[1].fill_between(bin_edges[:-1], ((mean_hist-std_hist)/(true_hist)), ((mean_hist+std_hist)/(true_hist)), step='post',alpha=.3, color='tab:orange')\n","ax[1].step(bin_edges[:-1], (mean_hist/(true_hist + 1e-10)), color='tab:orange',  where='post')\n","ax[1].axhline(y=1., color='black', linestyle='-')\n","ax[1].axhline(y=1.2, color='tab:gray', linestyle='--')\n","ax[1].axhline(y=0.8, color='tab:gray', linestyle='--')\n","\n","ax[2].set_ylabel(r\"$\\delta [\\%]$\")\n","ax[2].set_xlabel('truth amplitudes')\n","ax[2].set_ylim((0.05, 50))\n","ax[2].set_yscale(\"log\")\n","ax[2].set_yticks([0.1, 1.0, 10.0])\n","ax[2].set_yticklabels([r\"$0.1$\", r\"$1.0$\", \"$10.0$\"])\n","ax[2].set_yticks([0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9,\n","                               2., 3., 4., 5., 6., 7., 8., 9., 20., 30., 40.,], minor=True)\n","ax[2].axhline(y=1.0, linewidth=0.5, linestyle=\"--\", color=\"grey\")\n","ax[2].errorbar(bin_center, delta, color='k', linewidth=1.0, fmt=\".\", capsize=2)\n","\n","fig.tight_layout()"],"metadata":{"id":"curzkvvUX3oj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define a function which calculates the pull values\n","# given the prediction, true values, and the predicted variances\n","\n","def calculate_pulls(predictions, true_values, sigma_tot, sigma_stat, sigma_syst):\n","    # Your code here...\n","\n","bnn_pull_total, bnn_pull_stat, bnn_pull_syst =\n","re_pull_total, re_pull_stat, re_pull_syst ="],"metadata":{"id":"Y3Cns7uDZ5t0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from scipy.stats import norm\n","\n","fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n","\n","# Plot bnn pull distributions\n","bins = np.linspace(-5, 5, 50)\n","ax[0].hist(bnn_pull_total, bins=bins, density=True, alpha=0.6, label='BNN')\n","p = norm.pdf(bins, 0, 1)\n","ax[0].plot(bins, p, 'k--', linewidth=2, label='$\\mathcal{N}(0,1)$')\n","ax[0].set_xlabel(\"$\\\\frac{A - \\\\bar{A}}{\\\\sigma_{total}}$\")\n","ax[0].set_ylabel(\"Density\")\n","ax[0].legend()\n","\n","ax[1].hist(bnn_pull_syst, bins=bins, density=True, alpha=0.6, label='BNN')\n","ax[1].plot(bins, p, 'k--', linewidth=2, label='$\\mathcal{N}(0,1)$')\n","ax[1].set_xlabel(\"$\\\\frac{A - \\\\bar{A}}{\\\\sigma_{syst}}$\")\n","ax[1].set_ylabel(\"Density\")\n","ax[1].legend()\n","\n","ax[2].hist(bnn_pull_stat, bins=bins, density=True, alpha=0.6, label='BNN')\n","ax[2].plot(bins, p, 'k--', linewidth=2, label='$\\mathcal{N}(0,1)$')\n","ax[2].set_xlabel(\"$\\\\frac{A - \\\\bar{A}}{\\\\sigma_{stat}}$\")\n","ax[2].set_ylabel(\"Density\")\n","ax[2].legend()\n","\n","fig.tight_layout()\n","plt.show()\n"],"metadata":{"id":"A0lGLZffayT9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n","\n","# Plot re pull distributions\n","bins = np.linspace(-5, 5, 50)\n","ax[0].hist(re_pull_total, bins=bins, density=True, alpha=0.6, label='RE')\n","p = norm.pdf(bins, 0, 1)\n","ax[0].plot(bins, p, 'k--', linewidth=2, label='$\\mathcal{N}(0,1)$')\n","ax[0].set_xlabel(\"$\\\\frac{A - \\\\bar{A}}{\\\\sigma_{total}}$\")\n","ax[0].set_ylabel(\"Density\")\n","ax[0].legend()\n","\n","ax[1].hist(re_pull_syst, bins=bins, density=True, alpha=0.6, label='RE')\n","ax[1].plot(bins, p, 'k--', linewidth=2, label='$\\mathcal{N}(0,1)$')\n","ax[1].set_xlabel(\"$\\\\frac{A - \\\\bar{A}}{\\\\sigma_{syst}}$\")\n","ax[1].set_ylabel(\"Density\")\n","ax[1].legend()\n","\n","ax[2].hist(re_pull_stat, bins=bins, density=True, alpha=0.6, label='RE')\n","ax[2].plot(bins, p, 'k--', linewidth=2, label='$\\mathcal{N}(0,1)$')\n","ax[2].set_xlabel(\"$\\\\frac{A - \\\\bar{A}}{\\\\sigma_{stat}}$\")\n","ax[2].set_ylabel(\"Density\")\n","ax[2].legend()\n","\n","fig.tight_layout()\n","plt.show()\n"],"metadata":{"id":"d-C4wjhJ9z5D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Questions&Exercises"],"metadata":{"id":"t6S7ZvSrniYX"}},{"cell_type":"markdown","source":["  - Do you observe qualitative differences between the two approaches?\n","  - Has the neural network correctly learned the systematic noise? Hint: try to calculate the mean relative systematic ucertainty\n","  - Define your own noise and check whether the network can learn its distribution or not.\n","  - Investigate more carefully the repulsive ensemble (e.g. change the number of channels, the prior width, and the repulsive pre-factor)\n","  - Visualize predictions vs. a single kinematic variable: Choose one of the kinematic variables (e.g., leading photon $p_T$) and create a scatter plot showing the true amplitudes vs. this variable. This can help visualize how well the models are capturing the amplitude dependence on specific kinematics and how the uncertainties vary across the kinematic space."],"metadata":{"id":"a2oCLj9ga0kJ"}}],"metadata":{"colab":{"toc_visible":true,"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}